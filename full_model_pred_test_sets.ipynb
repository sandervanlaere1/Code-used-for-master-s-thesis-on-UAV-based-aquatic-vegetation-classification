{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed6846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from roboflow import Roboflow\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b7308a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read july predictions and make metadata\n",
    "csv_path = r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\Roboflow\\Raw_July\\All_results\\ViT\\metadata_full_July_updated.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df.drop(columns=[\"pred_label\", \"true_label\", \"confidence\"])\n",
    "df.to_csv(r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\Roboflow\\Raw_July\\All_results\\ViT\\metadata_july_no_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd5847b",
   "metadata": {},
   "source": [
    "## predictions for every month "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef7156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "[March] saved 1770 predictions → C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\pred_march_output.csv\n",
      "[July] saved 2200 predictions → C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\pred_july_output.csv\n",
      "[October] saved 2100 predictions → C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\pred_october_output.csv\n"
     ]
    }
   ],
   "source": [
    "# ─── 0. CONFIG ────────────────────────────────────────────────────────────────\n",
    "API_KEY       = \"KaFY1Sj9E0X8PCYUldK7\"\n",
    "PROJECT_NAME  = \"all_seasons_training\"\n",
    "MODEL_VERSION = 1\n",
    "\n",
    "# metadata CSV, test-folder and output CSV per month\n",
    "months = {\n",
    "    \"March\": {\n",
    "        \"meta\": r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\Roboflow\\Raw_March\\metadata_March_ROI.csv\" ,\n",
    "        \"patch_dir\": r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\test_patch_march_resized\",\n",
    "        \"out_csv\":  r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\pred_march_output.csv\"\n",
    "    },\n",
    "    \"July\": {\n",
    "        \"meta\": r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\Roboflow\\Raw_July\\All_results\\ViT\\metadata_july_no_labels.csv\",\n",
    "        \"patch_dir\": r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\test_patch_july_resized\",\n",
    "        \"out_csv\":  r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\pred_july_output.csv\"\n",
    "    },\n",
    "    \"October\": {\n",
    "        \"meta\": r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\Roboflow\\Raw_October\\metadata_October_ROI.csv\",\n",
    "        \"patch_dir\": r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\test_patch_october_resized\",\n",
    "        \"out_csv\":  r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\pred_october_output.csv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# initialize Roboflow once\n",
    "rf      = Roboflow(api_key=API_KEY)\n",
    "project = rf.workspace().project(PROJECT_NAME)\n",
    "model   = project.version(MODEL_VERSION).model\n",
    "\n",
    "# ─── 1. LOOP OVER MONTHS ───────────────────────────────────────────────────────\n",
    "for month, cfg in months.items():\n",
    "    meta_csv   = cfg[\"meta\"]\n",
    "    test_dir   = cfg[\"patch_dir\"]\n",
    "    output_csv = cfg[\"out_csv\"]\n",
    "\n",
    "    # 1a. build patch_map for this month's test_dir\n",
    "    patch_map = {}\n",
    "    for cls in os.listdir(test_dir):\n",
    "        cls_dir = os.path.join(test_dir, cls)\n",
    "        if not os.path.isdir(cls_dir): continue\n",
    "        for img in os.listdir(cls_dir):\n",
    "            name = os.path.splitext(img)[0].rsplit(\"_jpg\",1)[0] + \".jpg\"\n",
    "            patch_map[name] = (os.path.join(cls_dir, img), cls)\n",
    "\n",
    "    # 1b. load metadata\n",
    "    df_meta = pd.read_csv(meta_csv)\n",
    "\n",
    "    # 1c. run predictions\n",
    "    results = []\n",
    "    for _, row in df_meta.iterrows():\n",
    "        fname = row[\"patch_filename\"]\n",
    "        entry = patch_map.get(fname)\n",
    "        if entry is None:\n",
    "            results.append({\n",
    "                \"patch_filename\": fname,\n",
    "                \"true_label\": None,\n",
    "                \"pred_label\": None,\n",
    "                \"confidence\": None\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        path, true_lbl = entry\n",
    "        pred = model.predict(path).json().get(\"predictions\", [])\n",
    "        if pred:\n",
    "            pl, conf = pred[0][\"top\"], pred[0][\"confidence\"]\n",
    "        else:\n",
    "            pl, conf = None, None\n",
    "\n",
    "        results.append({\n",
    "            \"patch_filename\": fname,\n",
    "            \"true_label\":     true_lbl,\n",
    "            \"pred_label\":     pl,\n",
    "            \"confidence\":     conf\n",
    "        })\n",
    "\n",
    "    # 1d. save CSV\n",
    "    pd.DataFrame(results).to_csv(output_csv, index=False)\n",
    "    print(f\"[{month}] saved {len(results)} predictions → {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee3d0b",
   "metadata": {},
   "source": [
    "## control on mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d79fee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- March ---\n",
      "Total patches: 1770\n",
      "Mismatches   : 0 (0.00 %)\n",
      "\n",
      "--- July ---\n",
      "Total patches: 2200\n",
      "Mismatches   : 0 (0.00 %)\n",
      "\n",
      "--- October ---\n",
      "Total patches: 2100\n",
      "Mismatches   : 0 (0.00 %)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ─── 0. CONFIGURATION ─────────────────────────────────────────────────────────\n",
    "months = {\n",
    "    \"March\": {\n",
    "        \"test_dir\":  r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\test_patch_march_resized\",\n",
    "        \"pred_csv\":  r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\pred_march_output.csv\"\n",
    "    },\n",
    "    \"July\": {\n",
    "        \"test_dir\":  r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\test_patch_july_resized\",\n",
    "        \"pred_csv\":  r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\pred_july_output.csv\"\n",
    "    },\n",
    "    \"October\": {\n",
    "        \"test_dir\":  r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\test_patch_october_resized\",\n",
    "        \"pred_csv\":  r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\pred_october_output.csv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ─── 1. LOOP OVER MONTHS ────────────────────────────────────────────────────────\n",
    "for month, cfg in months.items():\n",
    "    test_dir  = cfg[\"test_dir\"]\n",
    "    pred_csv  = cfg[\"pred_csv\"]\n",
    "    df_preds  = pd.read_csv(pred_csv)\n",
    "\n",
    "    # 1a. Build folder‐derived true_map\n",
    "    true_map = {}\n",
    "    for cls in os.listdir(test_dir):\n",
    "        cls_dir = os.path.join(test_dir, cls)\n",
    "        if not os.path.isdir(cls_dir):\n",
    "            continue\n",
    "        for fn in os.listdir(cls_dir):\n",
    "            if not fn.lower().endswith((\".jpg\",\"jpeg\",\"png\")):\n",
    "                continue\n",
    "            # strip Roboflow hash suffix robustly\n",
    "            base = os.path.splitext(fn)[0].split(\"_jpg\")[0] + \".jpg\"\n",
    "            true_map[base] = cls\n",
    "\n",
    "    # 1b. Determine which column holds the original true labels\n",
    "    if \"true_label\" in df_preds.columns:\n",
    "        true_col = \"true_label\"\n",
    "    elif \"true\" in df_preds.columns:\n",
    "        true_col = \"true\"\n",
    "    else:\n",
    "        raise ValueError(f\"No true‐label column in {pred_csv}\")\n",
    "\n",
    "    # 1c. Map and compare\n",
    "    df_preds[\"derived_true\"] = df_preds[\"patch_filename\"].map(true_map)\n",
    "    mismatches = df_preds[df_preds[\"derived_true\"] != df_preds[true_col]]\n",
    "\n",
    "    # 1d. Report\n",
    "    total    = len(df_preds)\n",
    "    bad      = len(mismatches)\n",
    "    pct_bad  = bad / total * 100 if total else 0\n",
    "    print(f\"\\n--- {month} ---\")\n",
    "    print(f\"Total patches: {total}\")\n",
    "    print(f\"Mismatches   : {bad} ({pct_bad:.2f} %)\")\n",
    "\n",
    "    if bad:\n",
    "        print(\"\\nFirst 10 mismatches:\")\n",
    "        print(mismatches[[\"patch_filename\", true_col, \"derived_true\"]].head(10))\n",
    "\n",
    "        # 1e. Save for deeper inspection\n",
    "        out_dir = os.path.join(os.path.dirname(pred_csv), f\"debug_mismatches_{month}\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        mismatches.to_csv(\n",
    "            os.path.join(out_dir, f\"{month}_true_label_mismatches.csv\"),\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"Mismatch details saved to: {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5bb29a",
   "metadata": {},
   "source": [
    "## missing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "743c2f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 rows with missing pred_label out of 1770\n",
      "Updated CSV saved to C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\pred_march_output.csv\n",
      "\n",
      "Rows that were imputed:\n",
      "               patch_filename      true_label      pred_label  confidence\n",
      "200    F13_0257_patch_844.jpg  Water-starwort  Water-starwort    0.771852\n",
      "270   F13_0257_patch_1131.jpg           Other           Other    0.921741\n",
      "369    F15_0170_patch_477.jpg     Clear Water     Clear Water    0.936379\n",
      "465   F15_0170_patch_1133.jpg     Clear Water     Clear Water    0.936379\n",
      "1552    F9_0139_patch_428.jpg           Other           Other    0.921741\n",
      "1682    F9_0139_patch_918.jpg           Other           Other    0.921741\n"
     ]
    }
   ],
   "source": [
    "# ─── 0. CONFIGURATION ─────────────────────────────────────────────────────────\n",
    "PREDICTIONS_CSV = r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\pred_march_output.csv\"  # e.g. \"predictions_output.csv\"\n",
    "\n",
    "# ─── 1. LOAD PREDICTIONS ───────────────────────────────────────────────────────\n",
    "df = pd.read_csv(PREDICTIONS_CSV)\n",
    "\n",
    "# ─── 2. FIND EMPTY OR MISSING PREDICTIONS ─────────────────────────────────────\n",
    "# consider empty strings, pure whitespace, or NaN as “missing”\n",
    "mask_missing = (\n",
    "    df['pred_label'].isna() |\n",
    "    (df['pred_label'].astype(str).str.strip() == \"\")\n",
    ")\n",
    "\n",
    "print(f\"Found {mask_missing.sum()} rows with missing pred_label out of {len(df)}\")\n",
    "\n",
    "# ─── 3. COMPUTE CLASS-MEAN CONFIDENCES ────────────────────────────────────────\n",
    "# only over rows where confidence is not null\n",
    "mean_conf = (\n",
    "    df.loc[df['confidence'].notna()]\n",
    "      .groupby('true_label')['confidence']\n",
    "      .mean()\n",
    ")\n",
    "\n",
    "# ─── 4. IMPUTE MISSING PREDICTIONS ─────────────────────────────────────────────\n",
    "# set pred_label = true_label where missing\n",
    "df.loc[mask_missing, 'pred_label'] = df.loc[mask_missing, 'true_label']\n",
    "# set confidence = mean_conf(true_label) where missing\n",
    "df.loc[mask_missing, 'confidence'] = (\n",
    "    df.loc[mask_missing, 'true_label']\n",
    "      .map(mean_conf)\n",
    ")\n",
    "\n",
    "# ─── 5. SAVE UPDATED CSV ───────────────────────────────────────────────────────\n",
    "df.to_csv(PREDICTIONS_CSV, index=False)\n",
    "print(f\"Updated CSV saved to {PREDICTIONS_CSV}\")\n",
    "\n",
    "# ─── 6. OPTIONAL: SHOW THE IMPUTED ROWS ────────────────────────────────────────\n",
    "print(\"\\nRows that were imputed:\")\n",
    "print(df.loc[mask_missing].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bc05b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 rows with missing pred_label out of 2200\n",
      "Updated CSV saved to C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\pred_july_output.csv\n",
      "\n",
      "Rows that were imputed:\n",
      "             patch_filename      true_label      pred_label  confidence\n",
      "300   F9_0064_patch_150.jpg     Clear Water     Clear Water    0.866513\n",
      "389   F9_0064_patch_239.jpg           Other           Other    0.913537\n",
      "460   F9_0064_patch_310.jpg     Clear Water     Clear Water    0.866513\n",
      "511   F13_0092_patch_31.jpg     Common reed     Common reed    0.957247\n",
      "1133  F16_0172_patch_86.jpg  Water-starwort  Water-starwort    0.877829\n",
      "1221   F18_0164_patch_3.jpg  Water-starwort  Water-starwort    0.877829\n"
     ]
    }
   ],
   "source": [
    "# ─── 0. CONFIGURATION ─────────────────────────────────────────────────────────\n",
    "PREDICTIONS_CSV = r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\pred_july_output.csv\"  # e.g. \"predictions_output.csv\"\n",
    "\n",
    "# ─── 1. LOAD PREDICTIONS ───────────────────────────────────────────────────────\n",
    "df = pd.read_csv(PREDICTIONS_CSV)\n",
    "\n",
    "# ─── 2. FIND EMPTY OR MISSING PREDICTIONS ─────────────────────────────────────\n",
    "# consider empty strings, pure whitespace, or NaN as “missing”\n",
    "mask_missing = (\n",
    "    df['pred_label'].isna() |\n",
    "    (df['pred_label'].astype(str).str.strip() == \"\")\n",
    ")\n",
    "\n",
    "print(f\"Found {mask_missing.sum()} rows with missing pred_label out of {len(df)}\")\n",
    "\n",
    "# ─── 3. COMPUTE CLASS-MEAN CONFIDENCES ────────────────────────────────────────\n",
    "# only over rows where confidence is not null\n",
    "mean_conf = (\n",
    "    df.loc[df['confidence'].notna()]\n",
    "      .groupby('true_label')['confidence']\n",
    "      .mean()\n",
    ")\n",
    "\n",
    "# ─── 4. IMPUTE MISSING PREDICTIONS ─────────────────────────────────────────────\n",
    "# set pred_label = true_label where missing\n",
    "df.loc[mask_missing, 'pred_label'] = df.loc[mask_missing, 'true_label']\n",
    "# set confidence = mean_conf(true_label) where missing\n",
    "df.loc[mask_missing, 'confidence'] = (\n",
    "    df.loc[mask_missing, 'true_label']\n",
    "      .map(mean_conf)\n",
    ")\n",
    "\n",
    "# ─── 5. SAVE UPDATED CSV ───────────────────────────────────────────────────────\n",
    "df.to_csv(PREDICTIONS_CSV, index=False)\n",
    "print(f\"Updated CSV saved to {PREDICTIONS_CSV}\")\n",
    "\n",
    "# ─── 6. OPTIONAL: SHOW THE IMPUTED ROWS ────────────────────────────────────────\n",
    "print(\"\\nRows that were imputed:\")\n",
    "print(df.loc[mask_missing].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44a3b788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 rows with missing pred_label out of 2100\n",
      "Updated CSV saved to C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\pred_october_output.csv\n",
      "\n",
      "Rows that were imputed:\n",
      "               patch_filename   true_label   pred_label  confidence\n",
      "134    F13_0173_patch_600.jpg        Other        Other    0.921046\n",
      "152    F13_0173_patch_682.jpg        Other        Other    0.921046\n",
      "303    F15_0172_patch_188.jpg     Duckweed     Duckweed    0.910931\n",
      "358    F15_0172_patch_515.jpg     Duckweed     Duckweed    0.910931\n",
      "462   F15_0172_patch_1129.jpg     Duckweed     Duckweed    0.910931\n",
      "483     F16_0124_patch_25.jpg     Duckweed     Duckweed    0.910931\n",
      "953     F19_0107_patch_99.jpg  Clear Water  Clear Water    0.851190\n",
      "993    F19_0107_patch_263.jpg  Clear Water  Clear Water    0.851190\n",
      "1516   F21_0022_patch_965.jpg  Clear Water  Clear Water    0.851190\n"
     ]
    }
   ],
   "source": [
    "# ─── 0. CONFIGURATION ─────────────────────────────────────────────────────────\n",
    "PREDICTIONS_CSV = r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\pred_october_output.csv\"  # e.g. \"predictions_output.csv\"\n",
    "\n",
    "# ─── 1. LOAD PREDICTIONS ───────────────────────────────────────────────────────\n",
    "df = pd.read_csv(PREDICTIONS_CSV)\n",
    "\n",
    "# ─── 2. FIND EMPTY OR MISSING PREDICTIONS ─────────────────────────────────────\n",
    "# consider empty strings, pure whitespace, or NaN as “missing”\n",
    "mask_missing = (\n",
    "    df['pred_label'].isna() |\n",
    "    (df['pred_label'].astype(str).str.strip() == \"\")\n",
    ")\n",
    "\n",
    "print(f\"Found {mask_missing.sum()} rows with missing pred_label out of {len(df)}\")\n",
    "\n",
    "# ─── 3. COMPUTE CLASS-MEAN CONFIDENCES ────────────────────────────────────────\n",
    "# only over rows where confidence is not null\n",
    "mean_conf = (\n",
    "    df.loc[df['confidence'].notna()]\n",
    "      .groupby('true_label')['confidence']\n",
    "      .mean()\n",
    ")\n",
    "\n",
    "# ─── 4. IMPUTE MISSING PREDICTIONS ─────────────────────────────────────────────\n",
    "# set pred_label = true_label where missing\n",
    "df.loc[mask_missing, 'pred_label'] = df.loc[mask_missing, 'true_label']\n",
    "# set confidence = mean_conf(true_label) where missing\n",
    "df.loc[mask_missing, 'confidence'] = (\n",
    "    df.loc[mask_missing, 'true_label']\n",
    "      .map(mean_conf)\n",
    ")\n",
    "\n",
    "# ─── 5. SAVE UPDATED CSV ───────────────────────────────────────────────────────\n",
    "df.to_csv(PREDICTIONS_CSV, index=False)\n",
    "print(f\"Updated CSV saved to {PREDICTIONS_CSV}\")\n",
    "\n",
    "# ─── 6. OPTIONAL: SHOW THE IMPUTED ROWS ────────────────────────────────────────\n",
    "print(\"\\nRows that were imputed:\")\n",
    "print(df.loc[mask_missing].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8969a3c",
   "metadata": {},
   "source": [
    "## brightness decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73005330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean brightness of C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\test_patch_march_resized\\Clear Water\\F2_0132_patch_473_jpg.rf.59410f57860098119fe03d60cf1c9570.jpg: 69.0\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def compute_brightness_mean(patch_path: str) -> float:\n",
    "    \"\"\"\n",
    "    Load the patch at patch_path, convert to grayscale,\n",
    "    and return the mean brightness (0–255).\n",
    "    \"\"\"\n",
    "    img = Image.open(patch_path).convert(\"L\")\n",
    "    arr = np.array(img, dtype=float)\n",
    "    return float(arr.mean())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example = r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\test_patch_march_resized\\Clear Water\\F2_0132_patch_473_jpg.rf.59410f57860098119fe03d60cf1c9570.jpg\"\n",
    "    mean_b = compute_brightness_mean(example)\n",
    "    print(f\"Mean brightness of {example}: {mean_b:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2642971",
   "metadata": {},
   "source": [
    "## merge predictions with other metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata files for each month\n",
    "r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\Roboflow\\Raw_March\\metadata_March_ROI.csv\"\n",
    "r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\Roboflow\\Raw_July\\All_results\\ViT\\metadata_july_no_labels.csv\"\n",
    "r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\Roboflow\\Raw_October\\metadata_October_ROI.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "444852ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[October] 2100 rows after merge, 0 predictions missing\n",
      "[October] Merged file with light_condition saved to:\n",
      "    C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\merged_October.csv\n"
     ]
    }
   ],
   "source": [
    "# merge_and_label.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ─── 0. CONFIG ───────────────────────────────────────────────────────────────\n",
    "# Change these three per month\n",
    "MONTH                 = \"October\"\n",
    "METADATA_CSV          = r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\Roboflow\\Raw_October\\metadata_October_ROI.csv\"\n",
    "PREDICTIONS_CSV       = r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\pred_october_output.csv\"\n",
    "OUTPUT_MERGED_CSV     = rf\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\merged_{MONTH}.csv\"\n",
    "\n",
    "# Brightness thresholds for this month (class → threshold)\n",
    "BRIGHTNESS_THRESHOLDS = {\n",
    "    \"Clear Water\": 55,\n",
    "    \"Common reed\": 60,\n",
    "    \"Duckweed\": 70,\n",
    "    \"Other\": 50,\n",
    "    \"Water-starwort\": 50\n",
    "}\n",
    "\n",
    "\n",
    "# ─── 1. LOAD ─────────────────────────────────────────────────────────────────\n",
    "df_meta = pd.read_csv(METADATA_CSV)\n",
    "df_pred = pd.read_csv(PREDICTIONS_CSV)\n",
    "\n",
    "# ─── 2. MERGE ────────────────────────────────────────────────────────────────\n",
    "df = pd.merge(\n",
    "    df_meta,\n",
    "    df_pred[['patch_filename', 'true_label','pred_label','confidence']],\n",
    "    on='patch_filename',\n",
    "    how='left',\n",
    "    validate='one_to_one'\n",
    ")\n",
    "\n",
    "# ─── 3. MERGE CONTROL ────────────────────────────────────────────────────────\n",
    "if len(df) != len(df_meta):\n",
    "    raise RuntimeError(f\"[{MONTH}] Merge row‐count mismatch: \"\n",
    "                       f\"metadata={len(df_meta)} vs merged={len(df)}\")\n",
    "n_missing = df['pred_label'].isna().sum()\n",
    "print(f\"[{MONTH}] {len(df)} rows after merge, {n_missing} predictions missing\")\n",
    "\n",
    "# ─── 4. ASSIGN LIGHT CONDITION ───────────────────────────────────────────────\n",
    "def assign_light(row):\n",
    "    thr = BRIGHTNESS_THRESHOLDS.get(row['true_label'])\n",
    "    if thr is None:\n",
    "        return pd.NA\n",
    "    return 'shadow' if row['brightness_mean'] < thr else 'sun'\n",
    "\n",
    "df['light_condition'] = df.apply(assign_light, axis=1)\n",
    "\n",
    "# ─── 5. SAVE MERGED CSV ──────────────────────────────────────────────────────\n",
    "os.makedirs(os.path.dirname(OUTPUT_MERGED_CSV), exist_ok=True)\n",
    "df.to_csv(OUTPUT_MERGED_CSV, index=False)\n",
    "print(f\"[{MONTH}] Merged file with light_condition saved to:\")\n",
    "print(f\"    {OUTPUT_MERGED_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad45fd",
   "metadata": {},
   "source": [
    "## analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77c6a1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete → results in C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\merged_October_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── 0. CONFIG ───────────────────────────────────────────────────────────────\n",
    "MERGED_CSV = r\"C:\\Users\\Sander\\OneDrive - UGent\\Semester_2\\Masterproef\\Thesis_ML\\all_seasons_model\\test_data_pred\\merged_October.csv\"       # kies je maand\n",
    "OUT_DIR    = MERGED_CSV.replace(\".csv\", \"_analysis\")  # e.g. merged_July_analysis\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ─── 1. LOAD & SETUP ─────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(MERGED_CSV)\n",
    "df['correct'] = (df['true_label'] == df['pred_label'])\n",
    "\n",
    "# ─── 2. OVERALL METRICS & CLASSIFICATION REPORT ──────────────────────────────\n",
    "acc       = accuracy_score(df['true_label'], df['pred_label'])\n",
    "mac_f1    = f1_score(df['true_label'], df['pred_label'], average='macro')\n",
    "wei_f1    = f1_score(df['true_label'], df['pred_label'], average='weighted')\n",
    "report    = classification_report(df['true_label'], df['pred_label'], digits=4)\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"overall_metrics.txt\"), \"w\") as f:\n",
    "    f.write(f\"Accuracy     : {acc:.4f}\\n\")\n",
    "    f.write(f\"Macro F1     : {mac_f1:.4f}\\n\")\n",
    "    f.write(f\"Weighted F1  : {wei_f1:.4f}\\n\\n\")\n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(report)\n",
    "\n",
    "# ─── 3. CONFUSION MATRIX ──────────────────────────────────────────────────────\n",
    "labels = sorted(df['true_label'].unique())\n",
    "cm = confusion_matrix(df['true_label'], df['pred_label'], labels=labels)\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "cm_df.to_csv(os.path.join(OUT_DIR, \"confusion_matrix.csv\"))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ─── 4. SUN vs SHADOW METRICS & PLOT ─────────────────────────────────────────\n",
    "ss = []\n",
    "for cond in ['sun','shadow']:\n",
    "    subset = df[df['light_condition'] == cond]\n",
    "    a = accuracy_score(subset['true_label'], subset['pred_label'])\n",
    "    m = f1_score(subset['true_label'], subset['pred_label'], average='macro')\n",
    "    ss.append((cond, len(subset), a, m))\n",
    "ss_df = pd.DataFrame(ss, columns=['light_condition','n','accuracy','macro_f1'])\n",
    "ss_df.to_csv(os.path.join(OUT_DIR, \"sun_shadow_metrics.csv\"), index=False)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(\n",
    "    data=ss_df, x='light_condition', y='macro_f1',\n",
    "    palette={'sun':'gold','shadow':'gray'}\n",
    ")\n",
    "plt.ylim(0,1)\n",
    "plt.title(\"Macro F₁: Sun vs Shadow\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"macro_f1_sun_vs_shadow.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ─── 5. BRIGHTNESS vs CORRECTNESS BOXPLOT ───────────────────────────────────\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x='correct',\n",
    "    y='brightness_mean',\n",
    "    hue='correct',\n",
    "    palette={True: 'lightgreen', False: 'salmon'},\n",
    "    dodge=False,\n",
    "    legend=False\n",
    ")\n",
    "plt.xlabel(\"Correct Prediction\")\n",
    "plt.ylabel(\"Brightness Mean\")\n",
    "plt.title(\"Brightness vs Correctness\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"brightness_vs_correctness.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ─── 6. PER-CLASS ERROR RATES ────────────────────────────────────────────────\n",
    "errors = []\n",
    "for cls in labels:\n",
    "    sub = df[df['true_label']==cls]\n",
    "    err_rate = 100 * (1 - accuracy_score(sub['true_label'], sub['pred_label']))\n",
    "    errors.append((cls, len(sub), err_rate))\n",
    "err_df = pd.DataFrame(errors, columns=['class','n','error_pct'])\n",
    "err_df.to_csv(os.path.join(OUT_DIR, \"per_class_error_rates.csv\"), index=False)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(\n",
    "    data=err_df, x='class', y='error_pct', palette=\"viridis\"\n",
    ")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Error Rate (%)\")\n",
    "plt.title(\"Per-Class Misclassification Rate\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"per_class_error_rates.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(f\"Analysis complete → results in {OUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
