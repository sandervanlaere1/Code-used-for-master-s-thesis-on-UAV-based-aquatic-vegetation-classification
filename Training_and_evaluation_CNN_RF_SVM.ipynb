{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfUSBUA_pfPo",
        "outputId": "3298ffd7-3c1b-4a83-a383-ed7b96756cac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za3YC83_E_MH"
      },
      "source": [
        "### Load packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a6awJOhFGyL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import copy\n",
        "import time\n",
        "from itertools import product\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gaqwCq7FbSw"
      },
      "source": [
        "### Reproducibility & Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPfB2VpWFgH7",
        "outputId": "b0fe08a9-c98a-4a29-9ac3-e265ccd7156a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZIwhL3XFpZy"
      },
      "source": [
        "### Define Folder Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXPbnJqrFvnB"
      },
      "outputs": [],
      "source": [
        "# Benchmark configuration\n",
        "benchmark_dir = '/content/drive/MyDrive/Thesis/benchmark_test/test_v5'\n",
        "save_root = '/content/drive/MyDrive/Thesis/results/'\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/Thesis/no_preprocessing/train'  # CHANGE this to your actual path\n",
        "val_dir   = '/content/drive/MyDrive/Thesis/no_preprocessing/valid'\n",
        "test_dir  = '/content/drive/MyDrive/Thesis/no_preprocessing/test'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def confidence_interval(data, confidence=0.95):\n",
        "    a = 1.0 * np.array(data)\n",
        "    n = len(a)\n",
        "    m = np.mean(a)\n",
        "    se = np.std(a, ddof=1) / np.sqrt(n)\n",
        "    h = se * 1.96  # 95% CI\n",
        "    return m, m - h, m + h\n"
      ],
      "metadata": {
        "id": "6BLa1XsWgHdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B-2kWhoGc15"
      },
      "source": [
        "### Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvlkEQupGhdG"
      },
      "outputs": [],
      "source": [
        "# Data augmentation transforms for training\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomAffine(degrees=0, scale=(0.9, 1.1), shear=10),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# For validation and test (no augmentation!)\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-ky58dkGleB"
      },
      "source": [
        "### Create Datasets & Dataloaders with ImageFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOxSAJPzGqCq",
        "outputId": "6fd1b37e-188f-4dba-9963-9c9571142d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes found: ['Clear Water', 'Common reed', 'Duckweed', 'Other', 'Water-starwort']\n",
            "Num classes = 5\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32  # Adjust as needed\n",
        "\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
        "val_dataset   = datasets.ImageFolder(val_dir,   transform=val_test_transforms)\n",
        "test_dataset  = datasets.ImageFolder(test_dir,  transform=val_test_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Classes (in alphabetical order by default)\n",
        "class_names = train_dataset.classes\n",
        "benchmark_class_names = val_dataset.classes\n",
        "num_classes = len(class_names)\n",
        "print(\"Classes found:\", class_names)\n",
        "print(f\"Num classes = {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_dataset = datasets.ImageFolder(\n",
        "    '/content/drive/MyDrive/Thesis/benchmark_test/test_v5',\n",
        "    transform=val_test_transforms  # Important: same transforms as validation/test\n",
        ")\n",
        "\n",
        "benchmark_loader = DataLoader(benchmark_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "benchmark_class_names = benchmark_dataset.classes\n"
      ],
      "metadata": {
        "id": "YMA4Lp1Sje5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mJlirczbV5C"
      },
      "source": [
        "### Define Models (ResNet, VGG, EfficientNet, DenseNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnNykRZhc26o"
      },
      "source": [
        "**ResNet50**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "b8OW7ZJ7HbHi"
      },
      "outputs": [],
      "source": [
        "def create_resnet50(num_classes):\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False  # freeze all layers initially\n",
        "    # Unfreeze last two residual blocks and classifier\n",
        "    for param in model.layer3.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.layer4.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "    # Replace final layer\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Er3KiEuO4KR"
      },
      "source": [
        "**VGG16**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Sr_4F50AO3xR"
      },
      "outputs": [],
      "source": [
        "def create_vgg16(num_classes, dropout_rate=0.4):\n",
        "    model = models.vgg16(pretrained=True)\n",
        "\n",
        "    # Freeze all parameters first\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    num_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Sequential(\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(num_features, num_classes)\n",
        "    )\n",
        "    # Optionally fine-tune the last conv block and final classifier layer\n",
        "    for param in model.features[20:].parameters():  # last conv block\n",
        "        param.requires_grad = True\n",
        "    for param in model.classifier[6].parameters():  # final layer\n",
        "        param.requires_grad = True\n",
        "    return model\n",
        "\n",
        "\n",
        "    # All layers in the new classifier are trainable by default — no need to set requires_grad again\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VGG19**\n"
      ],
      "metadata": {
        "id": "ntPJwsBxZHDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vgg19(num_classes):\n",
        "    model = models.vgg19(pretrained=True)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    # Replace the classifier head\n",
        "    num_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Sequential(\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(num_features, num_classes)\n",
        "    )\n",
        "    # Optionally fine-tune the last conv block and final classifier layer\n",
        "    for param in model.features[24:].parameters():  # last conv block\n",
        "        param.requires_grad = True\n",
        "    for param in model.classifier[6].parameters():  # final layer\n",
        "        param.requires_grad = True\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "QhBgTUL0ZOZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7PHPluzPNWc"
      },
      "source": [
        "**EfficientNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qDlwoEktPMf4"
      },
      "outputs": [],
      "source": [
        "def create_efficientnet_b0(num_classes):\n",
        "    model = models.efficientnet_b0(pretrained=True)\n",
        "    # Freeze all\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    # Unfreeze the last 2 blocks\n",
        "    for param in model.features[6:].parameters():\n",
        "        param.requires_grad = True\n",
        "    # Modify the classifier\n",
        "    num_ftrs = model.classifier[1].in_features\n",
        "    model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.2, inplace=True),\n",
        "    nn.Linear(num_ftrs, num_classes)\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3d97RntoSO2"
      },
      "source": [
        "**DenseNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dP2aYqrNoUny"
      },
      "outputs": [],
      "source": [
        "def create_densenet121(num_classes):\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    # Freeze all\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    # Replace final layer\n",
        "    num_ftrs = model.classifier.in_features\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(0.3),  # Add dropout\n",
        "        nn.Linear(num_ftrs, num_classes)\n",
        "    )\n",
        "    for param in model.features.denseblock4.parameters():\n",
        "        param.requires_grad = True\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4InLk6bYeP9a"
      },
      "source": [
        "**ResNet34**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWfhSb70ePDm"
      },
      "outputs": [],
      "source": [
        "def create_resnet34(num_classes):\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    # Freeze all layers\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    # Replace the classifier\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    # Optionally unfreeze last block\n",
        "    for param in model.layer3.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.layer4.parameters():\n",
        "        param.requires_grad = True\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GoogLeNet**"
      ],
      "metadata": {
        "id": "17uFPWb6ZXgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_googlenet(num_classes):\n",
        "    model = models.googlenet(pretrained=True, aux_logits=True)\n",
        "    model.aux1 = None\n",
        "    model.aux2 = None\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    # Replace the final fully connected layer\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "    # Unfreeze the final inception block and final layer\n",
        "    for param in model.inception5b.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "m0gbl1pbZcE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2ArtwY5dkV_"
      },
      "source": [
        "### Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy7dsw-aXJYa"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_macro_f1(y_true, y_pred):\n",
        "    return f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                val_loader,\n",
        "                criterion,\n",
        "                optimizer,\n",
        "                scheduler=None,\n",
        "                model_name=\"model\",\n",
        "                num_epochs=10,\n",
        "                patience=3):\n",
        "    model = model.to(device)\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_macro_f1 = 0.0\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_f1_scores = []\n",
        "    val_f1_scores = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            model.train() if phase == \"train\" else model.eval()\n",
        "            loader = train_loader if phase == \"train\" else val_loader\n",
        "\n",
        "            running_loss = 0.0\n",
        "            all_preds = []\n",
        "            all_labels = []\n",
        "\n",
        "            for inputs, labels in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == \"train\"):\n",
        "                    outputs = model(inputs)\n",
        "                    if isinstance(outputs, tuple):  # Handle GoogLeNet\n",
        "                        outputs = outputs[0]\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == \"train\":\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            epoch_loss = running_loss / len(loader.dataset)\n",
        "            epoch_macro_f1 = calculate_macro_f1(all_labels, all_preds)\n",
        "\n",
        "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} F1: {epoch_macro_f1:.4f}\")\n",
        "\n",
        "            if phase == \"train\":\n",
        "                train_losses.append(epoch_loss)\n",
        "                train_f1_scores.append(epoch_macro_f1)\n",
        "            else:\n",
        "                val_losses.append(epoch_loss)\n",
        "                val_f1_scores.append(epoch_macro_f1)\n",
        "\n",
        "                if epoch_macro_f1 > best_macro_f1:\n",
        "                    best_macro_f1 = epoch_macro_f1\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    torch.save(model.state_dict(), f\"/content/drive/MyDrive/Thesis/saved_models/best_{model_name}.pth\")\n",
        "\n",
        "                    df = pd.DataFrame({\n",
        "                        'train_loss': train_losses,\n",
        "                        'val_loss': val_losses,\n",
        "                        'train_f1': train_f1_scores,\n",
        "                        'val_f1': val_f1_scores\n",
        "                    })\n",
        "                    df.to_csv(f\"/content/drive/MyDrive/Thesis/saved_models/training_curves_{model_name}.csv\", index=False)\n",
        "                    print(f\"Saved best model and curves for {model_name}\")\n",
        "\n",
        "                if epoch_loss < best_val_loss:\n",
        "                    best_val_loss = epoch_loss\n",
        "                    epochs_no_improve = 0\n",
        "                else:\n",
        "                    epochs_no_improve += 1\n",
        "\n",
        "                if scheduler is not None:\n",
        "                    scheduler.step(epoch_loss)\n",
        "\n",
        "        print()\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "    print(f\"Training complete. Best Macro F1: {best_macro_f1:.4f}\")\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_losses, val_losses, train_f1_scores, val_f1_scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR1tR_NxhLvx"
      },
      "outputs": [],
      "source": [
        "# Calculate class weights\n",
        "class_counts = [len(os.listdir(os.path.join(train_dir, class_name))) for class_name in class_names]\n",
        "total_samples = sum(class_counts)\n",
        "class_weights = [total_samples/class_count for class_count in class_counts]\n",
        "class_weights[4] *= 1.2  # or a fixed custom weight\n",
        "# Convert to tensor\n",
        "class_weights = torch.FloatTensor(class_weights).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNd82wYzLmoF"
      },
      "source": [
        "### Example: Training Multiple Models & Comparing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models_dict = {\n",
        "    \"resnet50\": create_resnet50(num_classes),\n",
        "    \"vgg16\": create_vgg16(num_classes),\n",
        "    \"effb0\": create_efficientnet_b0(num_classes),\n",
        "    \"resnet34\": create_resnet34(num_classes),\n",
        "    \"dense121\": create_densenet121(num_classes),\n",
        "    \"vgg19\": create_vgg19(num_classes),\n",
        "    \"googlenet\": create_googlenet(num_classes),\n",
        "}"
      ],
      "metadata": {
        "id": "IQuBDmqBCe-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, weight=None):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        ce_loss = nn.CrossEntropyLoss(weight=self.weight, reduction='none')(input, target)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
        "        return focal_loss.mean()\n"
      ],
      "metadata": {
        "id": "1cRWw7izFrhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE-SU8aILj3P"
      },
      "outputs": [],
      "source": [
        "# Set up for training\n",
        "summary_results = []\n",
        "num_epochs = 30\n",
        "patience = 10\n",
        "\n",
        "# Model and hyperparameter grid\n",
        "model_name = \"googlenet\"\n",
        "hyperparams_grid = {\n",
        "    'learning_rate': [3e-4, 1e-4],\n",
        "    'weight_decay': [1e-4, 1e-5],\n",
        "    'label_smoothing': [0.0, 0.1],\n",
        "    'scheduler_patience': [2],\n",
        "    'criterion': ['ce'],\n",
        "}\n",
        "\n",
        "hyperparam_combinations = list(product(*hyperparams_grid.values()))\n",
        "\n",
        "for i, (lr, wd, ls, sched_pat, crit_type) in enumerate(hyperparam_combinations):\n",
        "    run_name = f\"{model_name}_lr{lr}_wd{wd}_ls{ls}_crit{crit_type}\"\n",
        "    print(f\"\\n>> Run {i+1}/{len(hyperparam_combinations)}: {run_name}\")\n",
        "\n",
        "    model_save_dir = os.path.join(save_root, model_name, run_name)\n",
        "    os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "    model = create_googlenet(num_classes).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=wd)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=sched_pat, verbose=True)\n",
        "\n",
        "    if crit_type == 'ce':\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=ls)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Only 'ce' loss implemented so far\")\n",
        "\n",
        "    best_model, train_losses, val_losses, train_f1, val_f1 = train_model(\n",
        "        model, train_loader, val_loader, criterion, optimizer,\n",
        "        scheduler=scheduler, model_name=run_name, num_epochs=num_epochs, patience=patience\n",
        "    )\n",
        "\n",
        "    torch.save(best_model.state_dict(), os.path.join(model_save_dir, \"best_model.pth\"))\n",
        "    df_curves = pd.DataFrame({\n",
        "        'train_loss': train_losses,\n",
        "        'val_loss': val_losses,\n",
        "        'train_f1': train_f1,\n",
        "        'val_f1': val_f1\n",
        "    })\n",
        "    df_curves.to_csv(os.path.join(model_save_dir, \"training_curves.csv\"), index=False)\n",
        "\n",
        "    epochs_range = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, train_losses, label=\"Train Loss\")\n",
        "    plt.plot(epochs_range, val_losses, label=\"Val Loss\")\n",
        "    plt.title(\"Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, train_f1, label=\"Train F1\")\n",
        "    plt.plot(epochs_range, val_f1, label=\"Val F1\")\n",
        "    plt.title(\"Macro F1\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Macro F1\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(model_save_dir, \"training_plot.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # Evaluate on internal test set\n",
        "    best_model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = best_model(inputs)\n",
        "            if isinstance(outputs, tuple):\n",
        "                outputs = outputs[0]\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    internal_acc = accuracy_score(all_labels, all_preds)\n",
        "    internal_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    with open(os.path.join(model_save_dir, \"results_summary.txt\"), \"w\") as f:\n",
        "        f.write(f\"Internal Test Set:\\n\")\n",
        "        f.write(f\"Accuracy: {internal_acc:.4f}\\n\")\n",
        "        f.write(f\"Macro F1: {internal_f1:.4f}\\n\\n\")\n",
        "\n",
        "    # Evaluate on benchmark set\n",
        "    all_preds, all_labels, all_confidences, wrong_patches = [], [], [], []\n",
        "    total_inference_time, total_images = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in benchmark_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            start_time = time.time()\n",
        "            outputs = best_model(inputs)\n",
        "            batch_paths = [benchmark_loader.dataset.samples[i][0] for i in range(total_images, total_images + inputs.size(0))]\n",
        "            end_time = time.time()\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            confidences, preds = torch.max(probs, dim=1)\n",
        "            for true_label, pred_label, conf, path in zip(labels.cpu().numpy(), preds.cpu().numpy(), confidences.cpu().numpy(), batch_paths):\n",
        "                if true_label != pred_label:\n",
        "                    wrong_patches.append({\n",
        "                        \"true\": benchmark_class_names[true_label],\n",
        "                        \"pred\": benchmark_class_names[pred_label],\n",
        "                        \"confidence\": conf,\n",
        "                        \"path\": path\n",
        "                    })\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_confidences.extend(confidences.cpu().numpy())\n",
        "            total_inference_time += (end_time - start_time)\n",
        "            total_images += inputs.size(0)\n",
        "\n",
        "    conf_df = pd.DataFrame({\n",
        "        \"confidence\": all_confidences,\n",
        "        \"true\": [benchmark_class_names[true] for true, _ in zip(all_labels, all_preds)],\n",
        "        \"pred\": [benchmark_class_names[pred] for _, pred in zip(all_labels, all_preds)],\n",
        "    })\n",
        "    conf_df[\"correct\"] = conf_df[\"true\"] == conf_df[\"pred\"]\n",
        "\n",
        "    conf_df.to_csv(os.path.join(model_save_dir, \"confidence_scores.csv\"), index=False)\n",
        "    df_wrong = pd.DataFrame(wrong_patches)\n",
        "    df_wrong.to_csv(os.path.join(model_save_dir, \"wrong_predictions.csv\"), index=False)\n",
        "\n",
        "    benchmark_acc = accuracy_score(all_labels, all_preds)\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    weighted_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    acc_mean, acc_low, acc_high = confidence_interval([a == b for a, b in zip(all_labels, all_preds)])\n",
        "    avg_inference_time = (total_inference_time / total_images) * 1000\n",
        "\n",
        "    with open(os.path.join(model_save_dir, \"results_summary.txt\"), \"a\") as f:\n",
        "        f.write(f\"Benchmark Set:\\n\")\n",
        "        f.write(f\"Accuracy: {benchmark_acc:.4f} (95% CI: {acc_low:.4f}–{acc_high:.4f})\\n\")\n",
        "        f.write(f\"Macro F1: {macro_f1:.4f}\\n\")\n",
        "        f.write(f\"Weighted F1: {weighted_f1:.4f}\\n\")\n",
        "        f.write(f\"Precision: {precision:.4f}\\n\")\n",
        "        f.write(f\"Recall: {recall:.4f}\\n\")\n",
        "        f.write(f\"Avg Inference Time per Image: {avg_inference_time:.2f} ms\\n\")\n",
        "\n",
        "    cm_benchmark = confusion_matrix(all_labels, all_preds)\n",
        "    df_cm = pd.DataFrame(cm_benchmark, index=benchmark_class_names, columns=benchmark_class_names)\n",
        "    df_cm.to_csv(os.path.join(model_save_dir, \"benchmark_confusion_matrix.csv\"))\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(f\"{run_name} - Benchmark Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(model_save_dir, \"benchmark_confusion_matrix.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    summary_results.append({\n",
        "        \"model\": run_name,\n",
        "        \"accuracy\": benchmark_acc,\n",
        "        \"acc_low\": acc_low,\n",
        "        \"acc_high\": acc_high,\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"weighted_f1\": weighted_f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"inference_time_ms\": avg_inference_time\n",
        "    })\n",
        "\n",
        "# Final results\n",
        "df_summary = pd.DataFrame(summary_results)\n",
        "df_summary.to_csv(os.path.join(save_root, \"benchmark_summary_results.csv\"), index=False)\n",
        "df_summary.set_index('model')[['macro_f1', 'weighted_f1', 'accuracy']].plot(kind='bar', figsize=(10, 6))\n",
        "plt.title(\"Benchmark Performance Comparison\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(save_root, \"benchmark_model_comparison.png\"))\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Load the model architecture ===\n",
        "def create_resnet50(num_classes):\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
        "    for param in model.layer3.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.layer4.parameters():\n",
        "        param.requires_grad = True\n",
        "    return model\n",
        "\n",
        "# === Paths ===\n",
        "model_name = \"resnet50\"\n",
        "run_name = \"resnet50_lr5e-05_wd5e-05_ls0.1_critce\"\n",
        "model_path = f\"/content/drive/MyDrive/Thesis/saved_models/best_{run_name}.pth\"\n",
        "model_save_dir = f\"/content/drive/MyDrive/Thesis/results/{model_name}/{run_name}\"\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "# Load model\n",
        "model = create_resnet34(num_classes)\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# === Evaluate on internal test set ===\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        if isinstance(outputs, tuple): outputs = outputs[0]\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "internal_acc = accuracy_score(all_labels, all_preds)\n",
        "internal_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "with open(os.path.join(model_save_dir, \"results_summary.txt\"), \"w\") as f:\n",
        "    f.write(f\"Internal Test Set:\\n\")\n",
        "    f.write(f\"Accuracy: {internal_acc:.4f}\\n\")\n",
        "    f.write(f\"Macro F1: {internal_f1:.4f}\\n\\n\")\n",
        "\n",
        "# === Benchmark Evaluation ===\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_confidences = []\n",
        "wrong_patches = []\n",
        "total_inference_time = 0.0\n",
        "total_images = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in benchmark_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        start_time = time.time()\n",
        "        outputs = model(inputs)\n",
        "        end_time = time.time()\n",
        "\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        confidences, preds = torch.max(probs, dim=1)\n",
        "\n",
        "        batch_paths = [benchmark_loader.dataset.samples[i][0] for i in range(total_images, total_images + inputs.size(0))]\n",
        "        for true_label, pred_label, conf, path in zip(labels.cpu().numpy(), preds.cpu().numpy(), confidences.cpu().numpy(), batch_paths):\n",
        "            if true_label != pred_label:\n",
        "                wrong_patches.append({\n",
        "                    \"true\": benchmark_class_names[true_label],\n",
        "                    \"pred\": benchmark_class_names[pred_label],\n",
        "                    \"confidence\": conf,\n",
        "                    \"path\": path\n",
        "                })\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_confidences.extend(confidences.cpu().numpy())\n",
        "\n",
        "        total_images += inputs.size(0)\n",
        "        total_inference_time += (end_time - start_time)\n",
        "\n",
        "# === Save confidence histograms ===\n",
        "conf_df = pd.DataFrame({\n",
        "    \"confidence\": all_confidences,\n",
        "    \"true\": [benchmark_class_names[i] for i in all_labels],\n",
        "    \"pred\": [benchmark_class_names[i] for i in all_preds]\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=conf_df, x=\"confidence\", hue=\"true\", bins=30, kde=True, palette=\"tab10\")\n",
        "plt.title(\"Distribution of Prediction Confidence by True Class\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(model_save_dir, \"confidence_histogram_by_true_class.png\"))\n",
        "plt.close()\n",
        "\n",
        "conf_df[\"correct\"] = conf_df[\"true\"] == conf_df[\"pred\"]\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=conf_df, x=\"confidence\", hue=\"correct\", bins=30, kde=True, palette=\"Set2\")\n",
        "plt.title(\"Confidence: Correct vs Incorrect Predictions\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(model_save_dir, \"confidence_correct_vs_wrong.png\"))\n",
        "plt.close()\n",
        "\n",
        "# === Metrics ===\n",
        "benchmark_acc = accuracy_score(all_labels, all_preds)\n",
        "macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "weighted_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "precision = precision_score(all_labels, all_preds, average='macro')\n",
        "recall = recall_score(all_labels, all_preds, average='macro')\n",
        "acc_mean, acc_low, acc_high = confidence_interval([a == b for a, b in zip(all_labels, all_preds)])\n",
        "avg_inference_time = (total_inference_time / total_images) * 1000\n",
        "\n",
        "report = classification_report(all_labels, all_preds, target_names=benchmark_class_names, digits=4)\n",
        "with open(os.path.join(model_save_dir, \"classification_report_benchmark.txt\"), \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "cm_benchmark = confusion_matrix(all_labels, all_preds)\n",
        "df_cm = pd.DataFrame(cm_benchmark, index=benchmark_class_names, columns=benchmark_class_names)\n",
        "df_cm.to_csv(os.path.join(model_save_dir, \"benchmark_confusion_matrix.csv\"))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Benchmark Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(model_save_dir, \"benchmark_confusion_matrix.png\"))\n",
        "plt.close()\n",
        "\n",
        "# === Save Summary\n",
        "with open(os.path.join(model_save_dir, \"results_summary.txt\"), \"a\") as f:\n",
        "    f.write(f\"Benchmark Set:\\n\")\n",
        "    f.write(f\"Accuracy: {benchmark_acc:.4f} (95% CI: {acc_low:.4f}–{acc_high:.4f})\\n\")\n",
        "    f.write(f\"Macro F1: {macro_f1:.4f}\\n\")\n",
        "    f.write(f\"Weighted F1: {weighted_f1:.4f}\\n\")\n",
        "    f.write(f\"Precision: {precision:.4f}\\n\")\n",
        "    f.write(f\"Recall: {recall:.4f}\\n\")\n",
        "    f.write(f\"Avg Inference Time per Image: {avg_inference_time:.2f} ms\\n\")\n",
        "\n",
        "conf_df.to_csv(os.path.join(model_save_dir, \"confidence_scores.csv\"), index=False)\n",
        "pd.DataFrame(wrong_patches).to_csv(os.path.join(model_save_dir, \"wrong_predictions.csv\"), index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWm1zEM6DCYq",
        "outputId": "dce387dd-60fd-4555-894d-0fb4de6f3f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 0. MOUNT DRIVE & IMPORTS ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, classification_report, confusion_matrix\n",
        ")\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === 1. DEFINE VGG16 ARCHITECTURE ===\n",
        "def create_vgg16(num_classes, dropout_rate=0.4):\n",
        "    model = models.vgg16(pretrained=True)\n",
        "\n",
        "    # Freeze everything\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Replace final classifier layer with Dropout + Linear\n",
        "    num_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Sequential(\n",
        "        nn.Dropout(dropout_rate),\n",
        "        nn.Linear(num_features, num_classes)\n",
        "    )\n",
        "\n",
        "    # Unfreeze last conv block and the new classifier\n",
        "    for param in model.features[20:].parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.classifier[6].parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# === 2. PATHS & PARAMETERS ===\n",
        "base_dir      = \"/content/drive/MyDrive/Thesis/best_vgg16\"\n",
        "model_path    = os.path.join(base_dir, \"best_model.pth\")\n",
        "test_dir      = os.path.join(base_dir, \"Test_patches_original_march\")\n",
        "results_dir   = os.path.join(base_dir, \"evaluation_results_march\")\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# assume these are your class names in order\n",
        "class_names = sorted(os.listdir(test_dir))\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# === 3. DATASET & DATALOADER ===\n",
        "# adjust size to your patch resolution, e.g. 128x128\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
        "loader  = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# === 4. LOAD MODEL ===\n",
        "model = create_vgg16(num_classes)\n",
        "state = torch.load(model_path, map_location=device)\n",
        "model.load_state_dict(state)\n",
        "model.to(device).eval()\n",
        "\n",
        "# === 5. EVALUATE ===\n",
        "all_preds    = []\n",
        "all_labels   = []\n",
        "all_confidences = []\n",
        "wrong_records  = []\n",
        "total_time = 0.0\n",
        "total_imgs = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        b = inputs.size(0)\n",
        "\n",
        "        t0 = time.time()\n",
        "        outputs = model(inputs)\n",
        "        t1 = time.time()\n",
        "\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        confs, preds = torch.max(probs, dim=1)\n",
        "\n",
        "        total_time += (t1 - t0)\n",
        "        total_imgs += b\n",
        "\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "        all_confidences.extend(confs.cpu().tolist())\n",
        "\n",
        "        # record wrong\n",
        "        paths = [dataset.samples[i][0] for i in range(total_imgs-b, total_imgs)]\n",
        "        for true, pred, conf, p in zip(labels.cpu(), preds.cpu(), confs.cpu(), paths):\n",
        "            if true != pred:\n",
        "                wrong_records.append({\n",
        "                    \"true\": class_names[true],\n",
        "                    \"pred\": class_names[pred],\n",
        "                    \"confidence\": float(conf),\n",
        "                    \"path\": p\n",
        "                })\n",
        "\n",
        "# === 6. METRICS & ARTIFACTS ===\n",
        "# Classification report\n",
        "report = classification_report(\n",
        "    all_labels, all_preds, target_names=class_names, digits=4\n",
        ")\n",
        "with open(os.path.join(results_dir, \"classification_report.txt\"), \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "df_cm.to_csv(os.path.join(results_dir, \"confusion_matrix.csv\"))\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Benchmark Confusion Matrix\")\n",
        "plt.savefig(os.path.join(results_dir, \"confusion_matrix.png\"))\n",
        "plt.close()\n",
        "\n",
        "# Summary metrics\n",
        "acc      = accuracy_score(all_labels, all_preds)\n",
        "mac_f1   = f1_score(all_labels, all_preds, average='macro')\n",
        "wei_f1   = f1_score(all_labels, all_preds, average='weighted')\n",
        "avg_time = (total_time/total_imgs)*1000  # ms\n",
        "\n",
        "with open(os.path.join(results_dir, \"results_summary.txt\"), \"w\") as f:\n",
        "    f.write(f\"Accuracy      : {acc:.4f}\\n\")\n",
        "    f.write(f\"Macro F1      : {mac_f1:.4f}\\n\")\n",
        "    f.write(f\"Weighted F1   : {wei_f1:.4f}\\n\")\n",
        "    f.write(f\"Avg inf time  : {avg_time:.2f} ms/image\\n\")\n",
        "\n",
        "# Save confidences\n",
        "pd.DataFrame({\n",
        "    \"true\":       [class_names[i] for i in all_labels],\n",
        "    \"pred\":       [class_names[i] for i in all_preds],\n",
        "    \"confidence\": all_confidences\n",
        "}).to_csv(os.path.join(results_dir, \"confidence_scores.csv\"), index=False)\n",
        "\n",
        "# Save wrong predictions\n",
        "pd.DataFrame(wrong_records).to_csv(\n",
        "    os.path.join(results_dir, \"wrong_predictions.csv\"), index=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOwDnYAPQH1k",
        "outputId": "62e5f7c8-f0dd-476f-bf42-e5f35c59be8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:03<00:00, 164MB/s]\n",
            "<ipython-input-3-33bf8d981d69>:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(model_path, map_location=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, classification_report, confusion_matrix\n",
        ")\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# ─── 0. CONFIG ────────────────────────────────────────────────────────────────\n",
        "base_dir = \"/content/drive/MyDrive/Thesis/best_vgg16\"\n",
        "CONF_CSV    = os.path.join(base_dir, \"evaluation_results_march/confidence_scores.csv\")\n",
        "FILTERED_CSV = os.path.join(base_dir, \"confidence_scores_no_duckweed_commonreed.csv\")\n",
        "REPORT_TXT  = os.path.join(base_dir, \"classification_report_no_D_C.txt\")\n",
        "CM_CSV      = os.path.join(base_dir, \"confusion_matrix_no_D_C.csv\")\n",
        "CM_PNG      = os.path.join(base_dir, \"confusion_matrix_no_D_C.png\")\n",
        "\n",
        "# ─── 1. LOAD & FILTER ─────────────────────────────────────────────────────────\n",
        "df = pd.read_csv(CONF_CSV)\n",
        "\n",
        "# drop all rows whose true label is Duckweed or Common reed\n",
        "to_remove = [\"Duckweed\", \"Common reed\"]\n",
        "df_f = df[~df[\"true\"].isin(to_remove)].reset_index(drop=True)\n",
        "\n",
        "# save the filtered confidence scores\n",
        "df_f.to_csv(FILTERED_CSV, index=False)\n",
        "print(f\"Filtered confidence scores saved to {FILTERED_CSV}\")\n",
        "\n",
        "# ─── 2. RECALCULATE METRICS ───────────────────────────────────────────────────\n",
        "y_true = df_f[\"true\"]\n",
        "y_pred = df_f[\"pred\"]\n",
        "\n",
        "acc      = accuracy_score(y_true, y_pred)\n",
        "macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "weighted = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "\n",
        "# classification report, excluding the removed classes\n",
        "classes = sorted(df_f[\"true\"].unique())\n",
        "report = classification_report(y_true, y_pred,\n",
        "                               labels=classes,\n",
        "                               target_names=classes,\n",
        "                               digits=4)\n",
        "\n",
        "with open(REPORT_TXT, \"w\") as f:\n",
        "    f.write(f\"Accuracy    : {acc:.4f}\\n\")\n",
        "    f.write(f\"Macro F1    : {macro_f1:.4f}\\n\")\n",
        "    f.write(f\"Weighted F1 : {weighted:.4f}\\n\\n\")\n",
        "    f.write(\"Classification report by class:\\n\")\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"Classification report saved to {REPORT_TXT}\")\n",
        "\n",
        "# ─── 3. CONFUSION MATRIX ──────────────────────────────────────────────────────\n",
        "cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
        "df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
        "df_cm.to_csv(CM_CSV)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix (no Duckweed, no Common reed)\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(CM_PNG)\n",
        "plt.close()\n",
        "\n",
        "print(f\"Confusion matrix saved to {CM_CSV} and {CM_PNG}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcd8OYG1SRs2",
        "outputId": "e1931a7f-dd00-4def-920c-43ed2ad87d17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered confidence scores saved to /content/drive/MyDrive/Thesis/best_vgg16/confidence_scores_no_duckweed_commonreed.csv\n",
            "Classification report saved to /content/drive/MyDrive/Thesis/best_vgg16/classification_report_no_D_C.txt\n",
            "Confusion matrix saved to /content/drive/MyDrive/Thesis/best_vgg16/confusion_matrix_no_D_C.csv and /content/drive/MyDrive/Thesis/best_vgg16/confusion_matrix_no_D_C.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o94luv6fi56"
      },
      "source": [
        "## RF and SVM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-image\n"
      ],
      "metadata": {
        "id": "y2UsdulOnJsy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "148b1942-d040-4fb0-e7c1-a075bae8470e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.15.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from skimage.feature import graycomatrix, graycoprops\n",
        "from skimage import exposure\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import time"
      ],
      "metadata": {
        "id": "P0F0eJuYEsFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define folders (already correct in your setup)\n",
        "benchmark_dir = '/content/drive/MyDrive/Thesis/benchmark_test/test_v5'\n",
        "train_dir     = '/content/drive/MyDrive/Thesis/no_preprocessing/train'\n",
        "val_dir       = '/content/drive/MyDrive/Thesis/no_preprocessing/valid'\n",
        "test_dir      = '/content/drive/MyDrive/Thesis/no_preprocessing/test'\n",
        "save_dir      = \"/content/drive/MyDrive/Thesis/results/traditional_models\""
      ],
      "metadata": {
        "id": "JW8aJTJgjeTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "ml_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),  # keeps original 0–1 float values\n",
        "])\n"
      ],
      "metadata": {
        "id": "H4pT8-mGjQpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset_ml     = ImageFolder(train_dir, transform=ml_transforms)\n",
        "val_dataset_ml       = ImageFolder(val_dir, transform=ml_transforms)\n",
        "test_dataset_ml      = ImageFolder(test_dir, transform=ml_transforms)\n",
        "benchmark_dataset_ml = ImageFolder(benchmark_dir, transform=ml_transforms)\n",
        "\n",
        "train_loader_ml     = DataLoader(train_dataset_ml, batch_size=32, shuffle=False, num_workers=2)\n",
        "val_loader_ml       = DataLoader(val_dataset_ml, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader_ml      = DataLoader(test_dataset_ml, batch_size=32, shuffle=False, num_workers=2)\n",
        "benchmark_loader_ml = DataLoader(benchmark_dataset_ml, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Update your class_names to match this loader\n",
        "class_names = train_dataset_ml.classes\n"
      ],
      "metadata": {
        "id": "Zdprih_HjbkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_extra_features(img_batch):\n",
        "    img_batch = img_batch.numpy()\n",
        "    n_samples = img_batch.shape[0]\n",
        "\n",
        "    mean_features = []\n",
        "    std_features = []\n",
        "    max_features = []\n",
        "    min_features = []\n",
        "\n",
        "    exg_features = []\n",
        "    exr_features = []\n",
        "    exg_exr_diff_features = []\n",
        "    cive_features = []\n",
        "    vari_features = []\n",
        "    ngrdi_features = []\n",
        "    gr_ratio_features = []\n",
        "    gb_ratio_features = []\n",
        "\n",
        "    glcm_contrast_features = []\n",
        "    glcm_homogeneity_features = []\n",
        "    glcm_energy_features = []\n",
        "    glcm_asm_features = []\n",
        "    glcm_dissimilarity_features = []\n",
        "    glcm_correlation_features = []\n",
        "\n",
        "    entropy_features = []\n",
        "\n",
        "    for img in img_batch:\n",
        "        r = img[0, :, :]\n",
        "        g = img[1, :, :]\n",
        "        b = img[2, :, :]\n",
        "\n",
        "        # Basic statistics\n",
        "        mean_features.append([np.mean(r), np.mean(g), np.mean(b)])\n",
        "        std_features.append([np.std(r), np.std(g), np.std(b)])\n",
        "        max_features.append([np.max(r), np.max(g), np.max(b)])\n",
        "        min_features.append([np.min(r), np.min(g), np.min(b)])\n",
        "\n",
        "        # Vegetation indices\n",
        "        exg = 2 * g - r - b\n",
        "        exg_mean = np.mean(exg)\n",
        "        exg_features.append([exg_mean])\n",
        "\n",
        "        exr = 1.4 * r - g\n",
        "        exr_mean = np.mean(exr)\n",
        "        exr_features.append([exr_mean])\n",
        "\n",
        "        exg_exr_diff_features.append([exg_mean - exr_mean])\n",
        "\n",
        "        vari = (g - r) / (g + r - b + 1e-10)\n",
        "        vari_features.append([np.mean(vari)])\n",
        "\n",
        "        ngrdi = (g - r) / (g + r + 1e-10)\n",
        "        ngrdi_features.append([np.mean(ngrdi)])\n",
        "\n",
        "        cive = 0.441 * r - 0.811 * g + 0.385 * b + 18.78745\n",
        "        cive_features.append([np.mean(cive)])\n",
        "\n",
        "        gr_ratio = r / (g + 1e-10)\n",
        "        gb_ratio = g / (b + 1e-10)\n",
        "        gr_ratio_features.append([np.mean(gr_ratio)])\n",
        "        gb_ratio_features.append([np.mean(gb_ratio)])\n",
        "\n",
        "        # GLCM texture (from rescaled green channel)\n",
        "        g_rescaled = exposure.rescale_intensity(g, out_range=(0, 255)).astype(np.uint8)\n",
        "        glcm = graycomatrix(g_rescaled, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n",
        "\n",
        "        glcm_contrast_features.append([graycoprops(glcm, 'contrast')[0, 0]])\n",
        "        glcm_homogeneity_features.append([graycoprops(glcm, 'homogeneity')[0, 0]])\n",
        "        glcm_energy_features.append([graycoprops(glcm, 'energy')[0, 0]])\n",
        "        glcm_asm_features.append([graycoprops(glcm, 'ASM')[0, 0]])\n",
        "        glcm_dissimilarity_features.append([graycoprops(glcm, 'dissimilarity')[0, 0]])\n",
        "        glcm_correlation_features.append([graycoprops(glcm, 'correlation')[0, 0]])\n",
        "\n",
        "        # Entropy (green channel histogram)\n",
        "        hist, _ = np.histogram(g_rescaled, bins=256, range=(0, 255), density=True)\n",
        "        hist = hist + 1e-10\n",
        "        entropy = -np.sum(hist * np.log2(hist))\n",
        "        entropy_features.append([entropy])\n",
        "\n",
        "    # Stack all features\n",
        "    all_features = np.hstack([\n",
        "        mean_features,\n",
        "        std_features,\n",
        "        max_features,\n",
        "        min_features,\n",
        "        exg_features,\n",
        "        exr_features,\n",
        "        exg_exr_diff_features,\n",
        "        cive_features,\n",
        "        vari_features,\n",
        "        ngrdi_features,\n",
        "        gr_ratio_features,\n",
        "        gb_ratio_features,\n",
        "        glcm_contrast_features,\n",
        "        glcm_homogeneity_features,\n",
        "        glcm_energy_features,\n",
        "        glcm_asm_features,\n",
        "        glcm_dissimilarity_features,\n",
        "        glcm_correlation_features,\n",
        "        entropy_features\n",
        "    ])\n",
        "\n",
        "    return all_features\n",
        "\n",
        "\n",
        "def flatten_dataset_with_features(loader):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        extras = compute_extra_features(inputs)\n",
        "        all_features.append(extras)\n",
        "        all_labels.append(labels.numpy())\n",
        "\n",
        "    X = np.vstack(all_features)\n",
        "    y = np.concatenate(all_labels)\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "fXyFGpxQBfyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = flatten_dataset_with_features(train_loader_ml)\n",
        "X_val, y_val     = flatten_dataset_with_features(val_loader_ml)\n",
        "X_test, y_test   = flatten_dataset_with_features(test_loader_ml)\n",
        "X_benchmark, y_benchmark = flatten_dataset_with_features(benchmark_loader_ml)\n",
        "\n"
      ],
      "metadata": {
        "id": "UC2_wCHMgtxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Grid Search\n",
        "rf_params = {\n",
        "    'n_estimators': [500, 600, 700],\n",
        "    'max_depth': [10, 30, 50, None],\n",
        "    'min_samples_split': [2, 5, 7],\n",
        "    'min_samples_leaf': [1,2,3],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'bootstrap': [True, False]\n",
        "\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf_grid = RandomizedSearchCV(rf, rf_params, n_iter = 1000, scoring='f1_macro', cv=5, verbose=3, n_jobs=-1)\n",
        "start = time.time()\n",
        "rf_grid.fit(X_train, y_train)\n",
        "end = time.time()\n",
        "print(f\"Time taken: {(end - start) / 60:.2f} minutes\")\n",
        "print(\"Best RF Params:\", rf_grid.best_params_)\n",
        "print(\"Best RF F1 Score:\", rf_grid.best_score_)\n",
        "best_rf = rf_grid.best_estimator_\n",
        "joblib.dump(rf_grid.best_estimator_, '/content/drive/MyDrive/Thesis/results/best_rf.pkl')\n",
        "\n",
        "with open('/content/drive/MyDrive/Thesis/results/rf_best_params.txt', 'w') as f:\n",
        "    f.write(str(rf_grid.best_params_))\n"
      ],
      "metadata": {
        "id": "U8fxhR0DBoSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c95da1b-fb55-4e56-d01d-a677048a6920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n",
            "Time taken: 133.34 minutes\n",
            "Best RF Params: {'n_estimators': 700, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 50, 'criterion': 'entropy', 'bootstrap': False}\n",
            "Best RF F1 Score: 0.8144446714753389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(rf_grid.cv_results_['params']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Tua8jjrflIl",
        "outputId": "87822f3b-97d3-4327-a067-cc2c6fa1f7ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM Grid Search\n",
        "svm_params = {\n",
        "    'svc__C': [0.01, 0.1, 1, 10, 100, 500],\n",
        "    'svc__gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
        "    'svc__kernel': ['linear', 'rbf', 'poly'],\n",
        "    'svc__degree': [2, 3]  # Used only if kernel='poly'\n",
        "}\n",
        "\n",
        "\n",
        "# === Initialize base model ===\n",
        "svm = make_pipeline(StandardScaler(), SVC(probability=True, random_state=42))\n",
        "\n",
        "# === Perform Randomized Search ===\n",
        "svm_grid = RandomizedSearchCV(\n",
        "    estimator=svm,\n",
        "    param_distributions=svm_params,\n",
        "    n_iter=200,\n",
        "    scoring='f1_macro',\n",
        "    cv=5,\n",
        "    verbose=3,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# === Fit model and measure time ===\n",
        "start = time.time()\n",
        "svm_grid.fit(X_train, y_train)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"⏱️ Time taken: {(end - start) / 60:.2f} minutes\")\n",
        "print(\"✅ Best SVM Params:\", svm_grid.best_params_)\n",
        "print(\"📊 Best Macro F1 Score:\", svm_grid.best_score_)\n",
        "\n",
        "# === Save best model and parameters ===\n",
        "best_svm = svm_grid.best_estimator_\n",
        "joblib.dump(best_svm, \"best_svm_model.pkl\")\n",
        "\n",
        "with open(\"svm_best_params.txt\", \"w\") as f:\n",
        "    f.write(str(svm_grid.best_params_))\n"
      ],
      "metadata": {
        "id": "JYN6HpRoBp04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c61e4b-0f51-44b7-bcc3-54955bfe081a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
            "⏱️ Time taken: 5.65 minutes\n",
            "✅ Best SVM Params: {'svc__kernel': 'linear', 'svc__gamma': 'auto', 'svc__degree': 2, 'svc__C': 1}\n",
            "📊 Best Macro F1 Score: 0.8479582008153004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X, y, set_name, model_name):\n",
        "    preds = model.predict(X)\n",
        "    acc = accuracy_score(y, preds)\n",
        "    f1 = f1_score(y, preds, average='macro')\n",
        "    report = classification_report(y, preds, target_names=class_names, output_dict=True)\n",
        "    cm = confusion_matrix(y, preds)\n",
        "\n",
        "    # Save report\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    report_df.to_csv(os.path.join(save_dir, f\"{model_name}_{set_name}_report.csv\"))\n",
        "\n",
        "    # Save confusion matrix\n",
        "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "    cm_df.to_csv(os.path.join(save_dir, f\"{model_name}_{set_name}_confusion_matrix.csv\"))\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(f\"{model_name} - Confusion Matrix ({set_name})\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, f\"{model_name}_{set_name}_confusion_matrix.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"[{model_name} on {set_name}] Accuracy: {acc:.4f} | Macro F1: {f1:.4f}\")\n",
        "    return acc, f1\n",
        "\n",
        "evaluate_model(best_rf, X_test, y_test, set_name=\"Test\", model_name=\"RF\")\n",
        "evaluate_model(best_rf, X_benchmark, y_benchmark, set_name=\"Benchmark\", model_name=\"RF\")\n",
        "evaluate_model(best_svm, X_test, y_test, set_name=\"Test\", model_name=\"SVM\")\n",
        "evaluate_model(best_svm, X_benchmark, y_benchmark, set_name=\"Benchmark\", model_name=\"SVM\")"
      ],
      "metadata": {
        "id": "Nut0MstOel_N",
        "outputId": "e536e4fe-4e4b-437c-f96a-a23a4d119821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'best_rf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-baf062ebed39>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_benchmark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_benchmark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Benchmark\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SVM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_rf' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}